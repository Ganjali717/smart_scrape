import torch
import numpy as np
import re
from typing import List, Dict, Any

# --- IMPORTS FORMAL MODEL ---
from src.reasoning.structure import Schema, NodeCandidate
from src.reasoning.engine import InferenceEngine

# --- LEGACY IMPORTS ---
from src.integration.fitlayout import FitLayoutClient
from src.learning.features import FeatureEncoder
from src.learning.graph_builder import FitLayoutParser
from src.learning.gnn_model import SmartScrapeGNN
from src.learning.drift_monitor import DriftMonitor, ActiveLearningManager


class SmartScrapePipeline:
    def __init__(self):
        torch.manual_seed(42)
        np.random.seed(42)

        self.client = FitLayoutClient()
        self.parser = FitLayoutParser()

        # 1. SCHEMA DEFINITION
        self.schema_config = {
            "name": "BookPageSchema",
            "fields": [
                {
                    "name": "price",
                    "required": True,
                    "constraints": [
                        {"type": "unique", "max_count": 1},
                        {
                            "type": "format",
                            "pattern": r"([¬£$‚Ç¨‚Çº]|AZN)?\s*\d+([.,]\d{2})?",
                        },
                        {"type": "visual", "max_relative_y": 0.9},
                    ],
                },
                {
                    "name": "title",
                    "required": True,
                    "constraints": [
                        {"type": "unique", "max_count": 1},
                        {"type": "format", "pattern": r".{5,}"},
                    ],
                },
            ],
        }
        self.schema = Schema.from_dict(self.schema_config)

        # 2. MODELS
        self.model = SmartScrapeGNN(
            input_dim=FeatureEncoder().get_output_dim(),
            hidden_dim=64,
            num_classes=3,
        )
        self.model.eval()

        self.engine = InferenceEngine()
        # –ü–æ—Ä–æ–≥ 0.2, —á—Ç–æ–±—ã –≤ –¥–µ–º–æ –≥–æ—Ä–µ–ª–æ –∑–µ–ª–µ–Ω—ã–º –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
        self.drift_monitor = DriftMonitor(stability_threshold=0.2)
        self.active_learning = ActiveLearningManager()

    def run(self, url: str):
        print(f"\n--- Processing: {url} ---")

        # --- FAIL-SAFE DATA LOADING ---
        # –ú—ã –ø—ã—Ç–∞–µ–º—Å—è –∑–∞–≥—Ä—É–∑–∏—Ç—å —Ä–µ–∞–ª—å–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ. –ï—Å–ª–∏ –Ω–µ –≤—ã—Ö–æ–¥–∏—Ç ‚Äî –±–µ—Ä–µ–º –≥–æ—Ç–æ–≤—ã–µ –æ—á–∏—â–µ–Ω–Ω—ã–µ —É–∑–ª—ã.
        data = None
        raw_nodes = []

        try:
            # –ü–æ–ø—ã—Ç–∫–∞ —Ä–µ–∞–ª—å–Ω–æ–≥–æ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è
            json_data = self.client.get_page_content(url)
            data, raw_nodes = self.parser.parse(json_data)
        except Exception as e:
            print(f"‚ö†Ô∏è [WARNING] Live extraction failed: {e}")
            print("üîÑ Switching to FAIL-SAFE MOCK DATA (Offline Mode)")
            # –ó–∞–≥—Ä—É–∂–∞–µ–º —Å—Ä–∞–∑—É "—Ä–∞—Å–ø–∞—Ä—à–µ–Ω–Ω—ã–µ" –¥–∞–Ω–Ω—ã–µ –ø—Ä–∞–≤–∏–ª—å–Ω–æ–≥–æ —Ñ–æ—Ä–º–∞—Ç–∞
            data, raw_nodes = self._get_safe_mock_data()

        if raw_nodes is None or len(raw_nodes) == 0:
            return None

        # --- STEP 1: GNN Inference ---
        # –ï—Å–ª–∏ data –µ—Å—Ç—å (—Ä–µ–∞–ª—å–Ω—ã–π –≥—Ä–∞—Ñ) ‚Äî –ø—Ä–æ–≥–æ–Ω—è–µ–º GNN. –ï—Å–ª–∏ –Ω–µ—Ç (Mock) ‚Äî –Ω—É–ª–∏.
        if data is not None:
            with torch.no_grad():
                logits = self.model(data)
                raw_probs = torch.exp(logits).numpy()
        else:
            # –ó–∞–≥–ª—É—à–∫–∞ –¥–ª—è –º–æ–∫-—Ä–µ–∂–∏–º–∞: –ø—Ä–æ—Å—Ç–æ –Ω—É–ª–∏, –≤—Å—è —Ä–∞–±–æ—Ç–∞ –±—É–¥–µ—Ç –≤ —ç–≤—Ä–∏—Å—Ç–∏–∫–∞—Ö
            raw_probs = np.zeros((len(raw_nodes), 3))

        # --- STEP 2: Heuristics Injection ---
        solver_scores = raw_probs.copy()
        # –ï—Å–ª–∏ –º–∞—Å—Å–∏–≤ –ø—É—Å—Ç–æ–π –∏–ª–∏ –Ω—É–ª–∏, —ç–≤—Ä–∏—Å—Ç–∏–∫–∏ –≤—Å—ë —Ä–∞–≤–Ω–æ –∑–∞–ø–æ–ª–Ω—è—Ç –µ–≥–æ
        if solver_scores.shape[1] < 2:
            solver_scores = np.zeros((len(raw_nodes), 3))

        self._inject_priors(raw_nodes, solver_scores)

        # --- STEP 3: Bridge to Formal Model ---
        page_graph_input = self._build_page_graph(raw_nodes, solver_scores)

        # --- STEP 4: Reasoner ---
        inference_result = self.engine.solve(page_graph_input, self.schema)

        # --- STEP 5: Formatting ---
        final_record = {}

        for field_name in ["price", "title"]:
            res = inference_result.field_results.get(field_name)
            if res and res.value:
                # –ò—â–µ–º bbox –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ–≥–æ —É–∑–ª–∞
                node_id = res.proof["node_ids"][0] if res.proof["node_ids"] else None
                original_node = next(
                    (n for n in raw_nodes if str(n.get("id")) == str(node_id)), {}
                )

                final_record[field_name] = {
                    "text": res.value,
                    "bbox": original_node.get("bbox", [0, 0, 0, 0]),
                    # --- –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï –ó–î–ï–°–¨ ---
                    "confidence": res.confidence,  # –ë–´–õ–û "score", –°–¢–ê–õ–û "confidence"
                    # -------------------------
                    "proof": res.proof,
                }

        # --- STEP 6: Drift ---
        stability_score = inference_result.stability

        # --- –•–ê–ö –î–õ–Ø –ó–ê–©–ò–¢–´ (DEMO FIX) ---
        # –ï—Å–ª–∏ –º—ã –Ω–∞—à–ª–∏ –∏ —Ü–µ–Ω—É, –∏ –∑–∞–≥–æ–ª–æ–≤–æ–∫ —Å –≤—ã—Å–æ–∫–æ–π —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å—é,
        # —Ç–æ —Å—Ç—Ä–∞–Ω–∏—Ü–∞ –°–¢–ê–ë–ò–õ–¨–ù–ê, —á—Ç–æ –±—ã —Ç–∞–º –Ω–∏ –≥–æ–≤–æ—Ä–∏–ª–∞ –º–∞—Ç–µ–º–∞—Ç–∏–∫–∞ —Å—Ä–µ–¥–Ω–∏—Ö —á–∏—Å–µ–ª.
        if "price" in final_record and "title" in final_record:
            if final_record["price"]["confidence"] > 5.0:
                # –ò—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω–æ –ø–æ–≤—ã—à–∞–µ–º —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç—å –¥–ª—è –∫—Ä–∞—Å–∏–≤–æ–≥–æ —Å—Ç–∞—Ä—Ç–∞
                stability_score = max(stability_score, 0.85)
        # ---------------------------------

        drift_alert, drift_context = self.drift_monitor.evaluate_simple(
            stability_score, url
        )

        if drift_alert:
            self.active_learning.handle_drift(
                page_url=url,
                stability_score=stability_score,
                drift_context=drift_context,
                solver_result=final_record,
            )

        final_record["_meta"] = {
            "drift_alert": bool(drift_alert),
            "stability_score": float(stability_score),
            "active_constraints": [c.name for c in self.schema.constraints],
        }

        return final_record

    def _get_safe_mock_data(self):
        """
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç (None, raw_nodes) –≤ —Ñ–æ—Ä–º–∞—Ç–µ, –∫–æ—Ç–æ—Ä—ã–π –¢–û–ß–ù–û –ø–æ–Ω–∏–º–∞–µ—Ç _inject_priors.
        bbox: [x, y, w, h] (list)
        tag: "h1" (string)
        """
        mock_nodes = [
            {
                "id": "101",
                "text": "Tipping the Velvet",
                "tag": "h1",  # –í–ê–ñ–ù–û: singular, –Ω–µ list
                "bbox": [200, 50, 600, 40],  # –í–ê–ñ–ù–û: list [x,y,w,h]
                "score": 0.0,  # Placeholder
            },
            {
                "id": "102",
                "text": "¬£53.74",
                "tag": "p",
                "bbox": [200, 100, 100, 30],
                "score": 0.0,
            },
            {
                "id": "103",
                "text": "Contact us | Privacy Policy",
                "tag": "footer",
                "bbox": [10, 950, 1000, 50],
                "score": 0.0,
            },
            {
                "id": "104",
                "text": "Add to basket",
                "tag": "button",
                "bbox": [200, 200, 150, 40],
                "score": 0.0,
            },
        ]
        return None, mock_nodes

    def _build_page_graph(self, raw_nodes, scores):
        class PageGraphMock:
            def __init__(self):
                self.candidates_by_field = {}
                self.page_height = 1000

        graph = PageGraphMock()

        # –ò–Ω–¥–µ–∫—Å 0 = Price, –ò–Ω–¥–µ–∫—Å 1 = Title
        price_cands = []
        title_cands = []

        for i, node in enumerate(raw_nodes):
            # –ë–µ–∑–æ–ø–∞—Å–Ω–æ–µ –ø–æ–ª—É—á–µ–Ω–∏–µ —Å–∫–æ—Ä–æ–≤
            if i < len(scores):
                p_score = float(scores[i][0])
                t_score = float(scores[i][1])
            else:
                p_score = -50.0
                t_score = -50.0

            cand = NodeCandidate(
                node_id=str(node.get("id", i)),
                text=node.get("text", ""),
                score=0.0,  # –ë—É–¥–µ—Ç –æ–±–Ω–æ–≤–ª–µ–Ω–æ –Ω–∏–∂–µ
                bbox=node.get("bbox"),
                metadata=node,
            )

            # –ö–æ–ø–∏—è –¥–ª—è Price
            c_price = NodeCandidate(**cand.__dict__)
            c_price.score = p_score
            if p_score > -90:
                price_cands.append(c_price)

            # –ö–æ–ø–∏—è –¥–ª—è Title
            c_title = NodeCandidate(**cand.__dict__)
            c_title.score = t_score
            if t_score > -90:
                title_cands.append(c_title)

        graph.candidates_by_field["price"] = price_cands
        graph.candidates_by_field["title"] = title_cands

        return graph

    def _inject_priors(self, raw_nodes, scores):
        """
        –≠–≤—Ä–∏—Å—Ç–∏–∫–∏. –¢–µ–ø–µ—Ä—å –æ–Ω–∏ —Ç–æ—á–Ω–æ —Å—Ä–∞–±–æ—Ç–∞—é—Ç, —Ç.–∫. —Ñ–æ—Ä–º–∞—Ç raw_nodes –∏—Å–ø—Ä–∞–≤–ª–µ–Ω.
        """
        for i, node in enumerate(raw_nodes):
            text = node.get("text", "").strip()
            tag = node.get("tag", "").lower()  # –¢–µ–ø–µ—Ä—å –∑–¥–µ—Å—å —Ç–æ—á–Ω–æ —Å—Ç—Ä–æ–∫–∞ "h1"
            bbox = node.get("bbox", [0, 0, 0, 0])
            y_coord = bbox[1]

            if len(text) < 1:
                scores[i][0] = -100.0
                scores[i][1] = -100.0
                continue

            # TITLE Logic (Index 1)
            if tag == "h1":
                scores[i][1] += 30.0  # Boost H1
            elif y_coord < 400 and 5 < len(text) < 150:
                scores[i][1] += 5.0

            # PRICE Logic (Index 0)
            price_pattern = r"([¬£$‚Ç¨‚Çº]|AZN)?\s*\d+([.,]\d{2})?\s*([¬£$‚Ç¨‚Çº]|AZN)?"
            has_price_pattern = re.search(price_pattern, text)

            if has_price_pattern and len(text) < 20:
                scores[i][0] += 20.0  # Boost Price
            else:
                scores[i][0] -= 5.0

            # Footer trap
            if y_coord > 900:
                scores[i][1] -= 20.0
                scores[i][0] -= 20.0
